# 数据结构与算法07-散列表

# 1.散列表简介

**散列表用的是数组支持按照下标随机访问数据的特性,所以散列表其实就是数组的一种扩展,由数组演化而来.**

![image-20210810174513196](https://fechin-picgo.oss-cn-shanghai.aliyuncs.com/PicGo/image-20210810174513196.png)

散列表用的就是数组支持按照下标随机访问的时候,时间复杂度是O(1)的特性.我们通过散列函数把元素的键值映射为下标,然后将数据存储在数组中对应下标的位置.当我们按照键值查询元素时,我们用同样的散列函数,将键值转化为数组下标,从对应的数组下标取数据;

# 2.散列函数

如何构造散列函数:

1. 散列函数计算得到的散列值是一个非负整数;
2. 如果key1=key2,那hash(key1)==hash(key2);
3. 如果key1!=key2,那hash(key1)!=hash(key2);(要想找到一个不同的key对应的散列值都不一样的散列函数,几乎不可能);

# 3.散列冲突

常用的散列冲突解决方法有两类,开发寻址法和链表法;

## 3.1.开放寻址法

核心思想是:如果出现了散列冲突,就重新探测一个空闲位置,将其插入.有一个比较简单的探测方法,**线性探测**;

当我们往散列表中插入数据时，如果某个数据经过散列函数散列之后，存储位置已经被占用了，我们就从当前位置开始，依次往后查找，看是否有空闲位置，直到找到为止.

![image-20210810180105020](https://fechin-picgo.oss-cn-shanghai.aliyuncs.com/PicGo/image-20210810180105020.png)

在散列表中查找元素的过程类似于插入的过程.我们通过散列函数求出要查找元素的键值对应的散列值，然后比较数组中下标为散列值的元素和要查找的元素。如果相等，则说明就是我们要找的元素；否则就顺序往后依次查找。如果遍历到数组中的空闲位置，还没有找到，就说明要查找的元素并没有在散列表中.

![image-20210810180309945](https://fechin-picgo.oss-cn-shanghai.aliyuncs.com/PicGo/image-20210810180309945.png)

对于使用线性探测法解决冲突的散列表,不能单纯地把要删除的元素设置为空.因为在查找的时候,一旦我们通过线性探测方法,找到一个空闲位置,就可以认定散列表不存在这个数据.如果这个空闲位置是我们后来删除的,就会导致原来的查找算法失效.我们可以将删除的元素，特殊标记为 deleted。当线性探测查找的时候，遇到标记为 deleted 的空间，并不是停下来，而是继续往下探测.

当散列表中插入的数据越来越多时，散列冲突发生的可能性就会越来越大，空闲位置会越来越少，线性探测的时间就会越来越久。极端情况下，我们可能需要探测整个散列表，所以最坏情况下的时间复杂度为 O(n)。同理，在删除和查找时，也有可能会线性探测整张散列表，才能找到要查找或者删除的数据.

开发寻址冲突解决方法,除了线性探测方法外,还有两种比较经典的探测方法;

* 二次探测:线性探测每次探测的步长是 1，那它探测的下标序列就是 hash(key)+0，hash(key)+1，hash(key)+2……而二次探测探测的步长就变成了原来的“二次方”，也就是说，它探测的下标序列就是 hash(key)+0，hash(key)+1^2，hash(key)+2^2……
* 双重散列:意思就是不仅要使用一个散列函数。我们使用一组散列函数 hash1(key)，hash2(key)，hash3(key)……我们先用第一个散列函数，如果计算得到的存储位置已经被占用，再用第二个散列函数，依次类推，直到找到空闲的存储位置。

不管使用哪种探测方法,当散列表中空闲位置不多的时候,散列冲突的概率会大大提高,为了尽可能保证散列表的操作效率.我们会尽可能保证散列表中有一定比例的空闲槽位。我们用**装载因子**（load factor）来表示空位的多少;`散列表的装载因子=填入表中的元素个数/散列表的长度`.装载因子越大，说明空闲位置越少，冲突越多，散列表的性能会下降。

## 3.2.链表法

![image-20210810182303020](https://fechin-picgo.oss-cn-shanghai.aliyuncs.com/PicGo/image-20210810182303020.png)

当插入的时候，我们只需要通过散列函数计算出对应的散列槽位，将其插入到对应链表中即可，所以插入的时间复杂度是 O(1)。当查找、删除一个元素时，我们同样通过散列函数计算出对应的槽，然后遍历链表查找或者删除.两个操作的时间复杂度跟链表的长度 k 成正比，也就是 O(k)。对于散列比较均匀的散列函数来说，理论上讲，k=n/m，其中 n 表示散列中数据的个数，m 表示散列表中“槽”的个数。

# 4.如何设计工业级的散列表

## 4.1.如何设计散列函数

1. 散列函数设计不能太复杂,过于复杂的散列函数,会消耗很多计算时间.直接影响到散列表的性能;
2. 散列函数生成的值尽可能随机并且均匀分布,这样才能避免或者最小化散列冲突.即便出现冲突,散列到每个槽的数据也会比较平均;

## 4.2.装载因子过大

针对散列表的扩容,数据搬移操作要复杂很多.因为散列表的大小变了.数据的存储位置也变了.所以我们要通过散列函数重新计算每个数据的存储位置.

![image-20210810184411372](https://fechin-picgo.oss-cn-shanghai.aliyuncs.com/PicGo/image-20210810184411372.png)

插入一个数据，最好情况下，不需要扩容，最好时间复杂度是 O(1)。最坏情况下，散列表装载因子过高，启动扩容，我们需要重新申请内存空间，重新计算哈希位置，并且搬移数据，所以时间复杂度是 O(n)。用摊还分析法，均摊情况下，时间复杂度接近最好情况，就是 O(1)。装载因子阈值需要选择得当。如果太大，会导致冲突过多；如果太小，会导致内存浪费严重。

> 装载因子 = 数据个数/槽的个数

装载因子阈值的设置要权衡时间、空间复杂度。如果内存空间不紧张，对执行效率要求很高，可以降低负载因子的阈值；相反，如果内存空间紧张，对执行效率要求又不高，可以增加负载因子的值，甚至可以大于 1。

## 4.3.避免低效扩容

如果散列表当前大小为 1GB，要想扩容为原来的两倍大小，那就需要对 1GB 的数据重新计算哈希值，并且从原来的散列表搬移到新的散列表.非常耗时;

为了解决一次性扩容耗时过多的情况，我们可以将扩容操作穿插在插入操作的过程中，分批完成。当装载因子触达阈值之后，我们只申请新空间，但并不将老的数据搬移到新散列表中。

当有新数据要插入时，我们将新数据插入新散列表中，并且从老的散列表中拿出一个数据放入到新散列表。每次插入一个数据到散列表，我们都重复上面的过程。经过多次插入操作之后，老的散列表中的数据就一点一点全部搬移到新散列表中了。这样没有了集中的一次性数据搬移，插入操作就都变得很快了。

对于查询操作，为了兼容了新、老散列表中的数据，我们先从新散列表中查找，如果没有找到，再去老的散列表中查找。

通过这样均摊的方法，将一次性扩容的代价，均摊到多次插入操作中，就避免了一次性扩容耗时过多的情况。这种实现方式，任何情况下，插入一个数据的时间复杂度都是 O(1)。

![image-20210810185224137](https://fechin-picgo.oss-cn-shanghai.aliyuncs.com/PicGo/image-20210810185224137.png)

## 4.4.选择冲突解决方法

### 4.4.1.开放寻址法

* 优点:开放寻址法不像链表法，需要拉很多链表。散列表中的数据都存储在数组中，可以有效地利用 CPU 缓存加快查询速度。而且，这种方法实现的散列表，序列化起来比较简单。链表法包含指针，序列化起来就没那么容易。
* 缺点:用开放寻址法解决冲突的散列表，删除数据的时候比较麻烦，需要特殊标记已经删除掉的数据。而且，在开放寻址法中，所有的数据都存储在一个数组中，比起链表法来说，冲突的代价更高。所以，使用开放寻址法解决冲突的散列表，装载因子的上限不能太大。这也导致这种方法比链表法更浪费内存空间。
* 当数据量比较小、装载因子小的时候，适合采用开放寻址法。这也是 Java 中的ThreadLocalMap使用开放寻址法解决散列冲突的原因。下面是ThreadLocalMap的部分源码

![image-20210810190353943](https://fechin-picgo.oss-cn-shanghai.aliyuncs.com/PicGo/image-20210810190353943.png)

### 4.4.2.链表法

* 基于链表的散列冲突处理方法比较适合存储大对象、大数据量的散列表，而且，比起开放寻址法，它更加灵活，支持更多的优化策略，比如用红黑树代替链表。

![image-20210810192131072](https://fechin-picgo.oss-cn-shanghai.aliyuncs.com/PicGo/image-20210810192131072.png)

如下是扩容的源码

~~~java
final Node<K,V>[] resize() {
        Node<K,V>[] oldTab = table;
        int oldCap = (oldTab == null) ? 0 : oldTab.length;
        int oldThr = threshold;
        int newCap, newThr = 0;
        if (oldCap > 0) {
            //容量和阈值都扩大成两倍
            newCap = oldCap << 1;
            newThr = oldThr << 1;
        }
        //设置阈值属性
        threshold = newThr;
        //新建一个是之前两倍容量的大小的Node数组
        Node<K,V>[] newTab = (Node<K,V>[])new Node[newCap];
        //属性赋值
        table = newTab;
        //完成一些准备，开始准备迁移之前节点
        if (oldTab != null) {
            //循环迁移每个节点数据
            for (int j = 0; j < oldCap; ++j) {
                Node<K,V> e;
                //并不数组中是每个位置都有元素
                if ((e = oldTab[j]) != null) {
                    //数据需要迁移，table相应位置置空
                    oldTab[j] = null;
                    //不存在链表情况
                    if (e.next == null)
                        //重新对新的容量快速取余，放入相应的位置
                        newTab[e.hash & (newCap - 1)] = e;
                    //树结构（较为复杂后续再分析）
                    else if (e instanceof TreeNode)
                        ((TreeNode<K,V>)e).split(this, newTab, j, oldCap);
                    //链表结构
                    else {
                        //不需要移动的链表的头尾指针
                        Node<K,V> loHead = null, loTail = null;
                        //需要移动的链表的头尾指针
                        Node<K,V> hiHead = null, hiTail = null;
                        Node<K,V> next;
                        //遍历链表将一个链表拆成两个链表，这里主要分析一下拆分依据
                        do {
                            next = e.next;
                            //扩容后余数与之前一致，不需要移动的节点。
                            if ((e.hash & oldCap) == 0) {
                                if (loTail == null)
                                    loHead = e;
                                else
                                    loTail.next = e;
                                loTail = e;
                            } else {
                                //扩容后余数与之前不一致，需要移动的节点。
                                if (hiTail == null)
                                    hiHead = e;
                                else
                                    hiTail.next = e;
                                hiTail = e;
                            }
                        } while ((e = next) != null);
                        //将链表重新放入table数组中
                        if (loTail != null) {
                            loTail.next = null;
                            newTab[j] = loHead;
                        }
                        if (hiTail != null) {
                            hiTail.next = null;
                            newTab[j + oldCap] = hiHead;
                        }
                    }
                }
            }
        }
        return newTab;
    }
~~~

# 5.为什么散列表总是与链表一起使用

散列表这种数据结构虽然支持非常高效的数据插入、删除、查找操作，但是散列表中的数据都是通过散列函数打乱之后无规律存储的。也就说，它无法支持按照某种顺序快速地遍历数据。如果希望按照顺序遍历散列表中的数据，那我们需要将散列表中的数据拷贝到数组中，然后排序，再遍历。因为散列表是动态数据结构，不停地有数据的插入、删除，所以每当我们希望按顺序遍历散列表中的数据的时候，都需要先排序，那效率势必会很低。为了解决这个问题，我们将散列表和链表（或者跳表）结合在一起使用。

## 5.1.LRU淘汰算法

我们需要维护一个按照访问时间从大到小有序排列的链表结构。因为缓存大小有限，当缓存空间不够，需要淘汰一个数据的时候，我们就直接将链表头部的结点删除。

当要缓存某个数据的时候，先在链表中查找这个数据。如果没有找到，则直接将数据放到链表的尾部；如果找到了，我们就把它移动到链表的尾部。因为查找数据需要遍历链表，所以单纯用链表实现的 LRU 缓存淘汰算法的时间复杂很高，是 O(n)。

![image-20211004234510782](https://fechin-picgo.oss-cn-shanghai.aliyuncs.com/PicGo/image-20211004234510782.png)

使用双向链表存储数据,链表中的每个结点处理存储数据(data),前驱指针(pre),后继指针(next),拉链指针(hnext);

一个链是刚刚我们提到的双向链表，另一个链是散列表中的拉链。前驱和后继指针是为了将结点串在双向链表中，hnext 指针是为了将结点串在散列表的拉链中。

这样查找数据,删除数据,添加数据,删除头节点,链表尾节点插入都可以在O(1)的时间复杂度内完成;

## 5.2.Redis有序集合

Redis有序集合的操作有如下:

* 添加一个成员对象；
* 按照键值来删除一个成员对象；
* 按照键值来查找一个成员对象；
* 按照分值区间查找数据，比如查找积分在[100, 356]之间的成员对象；
* 按照分值从小到大排序成员变量；

如果我们仅仅按照分值将成员对象组织成跳表的结构,那按照键值来删除,查询成员对象就会很慢.解决方法与 LRU 缓存淘汰算法的解决方法类似。我们可以再按照键值构建一个散列表，这样按照 key 来删除、查找一个成员对象的时间复杂度就变成了 O(1)。同时，借助跳表结构，其他操作也非常高效。

## 5.3.Java LinkedHashMap

LinkedHashMap中的Linked不仅仅表示它是链表法解决散列冲突.

~~~java
// 10是初始大小，0.75是装载因子，true是表示按照访问时间排序
HashMap<Integer, Integer> m = new LinkedHashMap<>(10, 0.75f, true);
m.put(3, 11);
m.put(1, 12);
m.put(5, 23);
m.put(2, 22);

m.put(3, 26);
m.get(5);

for (Map.Entry e : m.entrySet()) {
  System.out.println(e.getKey());//1,2,3,5;
}
~~~

每次调用`put()`函数往LinkedHashMap中添加数据的时候,都会将数据添加到链表的尾部,所以前四个操作完成之后,链表中的数据如下:

![image-20210810205344157](https://fechin-picgo.oss-cn-shanghai.aliyuncs.com/PicGo/image-20210810205344157.png)

在`map.put(3,26)`的代码中,再次将键值为3的数据放入到LinkedHashMap.会先查找这个键值是否已经有了,然后,再讲已经存在的(3,11)删除,并且将新的(3,26)放到链表的尾部:

![image-20210810210017642](https://fechin-picgo.oss-cn-shanghai.aliyuncs.com/PicGo/image-20210810210017642.png)

在`map.get(5)`的代码中,我们将被访问到的数据移动到链表的尾部.之后链表的数据如下:

![](https://fechin-picgo.oss-cn-shanghai.aliyuncs.com/PicGo/image-20210810210200164.png)

实际上,按照访问时间排序的LinkedHashMap本身就是一个支持LRU缓存淘汰策略的缓存系统.它们的实现原理也是一模一样;

# 6.位图

有1千万个整数,范围0-3千万,如何快速判定某个数据是否出现在这1千万个数据中;

![](https://fechin-picgo.oss-cn-shanghai.aliyuncs.com/PicGo/image-20211003180318805.png)

基于位图:**下标与数据一一映射**

```java
public class Mapping{
  private int maxValue = 30000000;
  private boolean a[] = new boolean[maxValue + 1];
  
  public void add(int data){
    a[data] = true;
  }
  
  public boolean contains(int data){
    return a[data];
  }
}
```

当然,在某些编程语言中,boolean类型并非只占用一个二进制位;

# 7.布隆过滤器

有1千万个整数,范围0-10亿,如何快速判定某个数据是否出现在这1千万个数据中?

* 如果使用位图的话,会让哈希冲突导致误判;

* 当判定为存在,实际上有可能不存在(会误判);当判定为不存在,就肯定不存在(不会误判);
* 存在冲突和误判的位图适用的场景:把这种存在冲突误判的位图,叫做**布隆过滤器**;

布隆过滤器降低冲突概率的方法:

* 用多个二进制位来表示一个数据;a,b经过3个哈希函数得到的值都相等的概率肯定要小很多;

![image-20211004164152073](https://fechin-picgo.oss-cn-shanghai.aliyuncs.com/PicGo/image-20211004164152073.png)

* 布隆过滤器会引入新的误判的情况如下:

![image-20211004164321036](https://fechin-picgo.oss-cn-shanghai.aliyuncs.com/PicGo/image-20211004164321036.png)

# 8.散列表的算法题

## 8.1.两数之和

给定一个整数数组nums和一个整数目标值target,请在该数组中找出和为目标值的两个整数,并返回它们的数组下标;

```java
class Solution{
  public int[] twoSum(int[] nums,int target){
    int n = nums.lenhth;
    //哈希表,key为数字本身,value为下标;
    //通过这个map可以快速地知道某个key是否存在,快速通过key获取value;
    HashMap<Integer,Integer> hashTable = new HashMap<>();
    for(int i = 0;i < n;i++){
      hashTable.put(nums[i],i);
    }
    
    for(int i = 0;i < n;i++){
      if(hashTable.containsKey(target-nums[i])){
        int value = hashTable.get(target-nums[i]);
        if(value != i){
          return new int[]{i,value};
        }
      }
    }
    return new int[0];
	}
}
```

## 8.2.实现LRU缓存

参考LinkedHashMap的实现
